{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36db953-ebb4-4818-a022-e79a2c91a2d0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761748591197}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ========================= #\n",
    "# ---------- LOAD --------- #\n",
    "# ========================= #\n",
    "from pyspark.sql.functions import col, year, month\n",
    "from functools import reduce\n",
    "\n",
    "# 1) List & Load tables (by prefix)\n",
    "tables = [t.name for t in spark.catalog.listTables(\"default\")]\n",
    "## filter with \"startswith\"\n",
    "dfs_2024 = [spark.table(f\"hive_metastore.default.{table}\") for table in tables if table.startswith(\"yellow_tripdata_2024\")]\n",
    "dfs_2025 = [spark.table(f\"hive_metastore.default.{table}\") for table in tables if table.startswith(\"yellow_tripdata_2025\")] \n",
    "\n",
    "# 2) Union per year\n",
    "df_2024 = reduce(lambda df1, df2: df1.unionByName(df2), dfs_2024) \n",
    "df_2025 = reduce(lambda df1, df2: df1.unionByName(df2), dfs_2025)\n",
    "\n",
    "# 3) Quick preview + counts\n",
    "display(df_2024)\n",
    "print(f\"rows in 2024 df: {df_2024.count()}\")\n",
    "print(f\"pickup in 2024 df: {df_2024.filter(year('tpep_pickup_datetime') == 2024).count()}\")\n",
    "print(f\"dropoff in 2024 df: {df_2024.filter(year('tpep_dropoff_datetime') == 2024).count()}\")\n",
    "print(f\"travel in 2024 df: {df_2024.filter((year('tpep_dropoff_datetime') == 2024) & (year('tpep_pickup_datetime') == 2024)).count()}\")\n",
    "\n",
    "null_expr_2024 = reduce(lambda x, y: x | y, [col(c).isNull() for c in df_2024.columns])\n",
    "print(f\"rows in 2024 df with at least one NULL: {df_2024.filter(null_expr_2024).count()}\")\n",
    "\n",
    "display(df_2025)\n",
    "print(f\"rows in 2025 df: {df_2025.count()}\")\n",
    "print(f\"pickup in 2025 df: {df_2025.filter(year('tpep_pickup_datetime') == 2025).count()}\")\n",
    "print(f\"dropoff in 2025 df: {df_2025.filter(year('tpep_dropoff_datetime') == 2025).count()}\")\n",
    "print(f\"travel in 2025 df: {df_2025.filter((year('tpep_dropoff_datetime') == 2025) & (year('tpep_pickup_datetime') == 2025)).count()}\")\n",
    "\n",
    "null_expr_2025 = reduce(lambda x, y: x | y, [col(c).isNull() for c in df_2025.columns])\n",
    "print(f\"rows in 2025 df with at least one NULL: {df_2025.filter(null_expr_2025).count()}\")\n",
    "\n",
    "# 4) Keep only good years/months\n",
    "##   - 2024: both pickup & dropoff in 2024\n",
    "##   - 2025: both in 2025 AND only months < 10 (Jan..Sep)\n",
    "df_2024 = df_2024.filter(\n",
    "    (year('tpep_dropoff_datetime') == 2024) & \n",
    "    (year('tpep_pickup_datetime') == 2024))\n",
    "\n",
    "df_2025 = df_2025.filter(\n",
    "    (year('tpep_dropoff_datetime') == 2025) &\n",
    "    (year('tpep_pickup_datetime') == 2025) &\n",
    "    (month('tpep_pickup_datetime')  < 10) &\n",
    "    (month('tpep_dropoff_datetime') < 10)\n",
    ")\n",
    "\n",
    "# 5) Union (2024 + 2025)\n",
    "dfs = [d for d in [df_2024, df_2025] if d is not None]\n",
    "result_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "print(f\"after union: {result_df.count()}\")\n",
    "\n",
    "## drop duplicates\n",
    "df = result_df.dropDuplicates()\n",
    "\n",
    "display(df)\n",
    "print(f\"df total rows: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8013e9-5a21-4082-bc41-0367f7f60c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66dcdc00-a7bb-41f2-af57-490bdee5c3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================== #\n",
    "# ----------- FILTER ------------ #\n",
    "# =============================== #\n",
    "from pyspark.sql.functions import col, trim, upper, to_timestamp, unix_timestamp, lit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 0) Base DF sanity (timestamps + duplicates)\n",
    "#    make sure timestamps are real timestamps, then drop exact duplicates\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\",  to_timestamp(\"tpep_pickup_datetime\"))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\"))\n",
    "     ).dropDuplicates()\n",
    "\n",
    "print(f\"[input] rows after typing+dedup: {df.count()}\")\n",
    "\n",
    "# 1) Load ref table (you created it) + minimal hygiene (trim/null/empty)\n",
    "ref_raw = spark.table(\"hive_metastore.default.taxi_universal_ref\")\n",
    "ref = (\n",
    "    ref_raw\n",
    "    .select(\n",
    "        trim(col(\"ref_type\")).alias(\"ref_type\"),\n",
    "        trim(col(\"code\")).alias(\"code\"),\n",
    "        trim(col(\"label\")).alias(\"label\")\n",
    "    )\n",
    "    .filter(col(\"ref_type\").isNotNull() & col(\"code\").isNotNull())\n",
    "    .filter((col(\"ref_type\") != \"\") & (col(\"code\") != \"\"))\n",
    "    .dropDuplicates([\"ref_type\",\"code\"])\n",
    ")\n",
    "\n",
    "# 2) Slices for SEMI filters (keep only code columns for performance)\n",
    "pay_ref_semi   = ref.filter(col(\"ref_type\")==\"payment_type\").select(col(\"code\").alias(\"code_pay\"))\n",
    "rate_ref_semi  = ref.filter(col(\"ref_type\")==\"RatecodeID\").select(col(\"code\").alias(\"code_rate\"))\n",
    "vend_ref_semi  = ref.filter(col(\"ref_type\")==\"VendorID\").select(col(\"code\").alias(\"code_vendor\"))\n",
    "store_ref_semi = ref.filter(col(\"ref_type\")==\"store_and_fwd_flag\").select(upper(col(\"code\")).alias(\"code_store\"))\n",
    "\n",
    "print(\"[refs] sizes (semi) →\",\n",
    "      \"payment:\", pay_ref_semi.count(),\n",
    "      \"| rate:\", rate_ref_semi.count(),\n",
    "      \"| vendor:\", vend_ref_semi.count(),\n",
    "      \"| store:\", store_ref_semi.count())\n",
    "\n",
    "# 3) LEFT SEMI filtering (keeps rows that match the code lists; does not duplicate rows)\n",
    "clean_df = df\n",
    "\n",
    "# VendorID\n",
    "clean_df = clean_df.join(F.broadcast(vend_ref_semi),\n",
    "                         clean_df.VendorID.cast(\"string\") == col(\"code_vendor\"),\n",
    "                         \"left_semi\")\n",
    "\n",
    "# RatecodeID\n",
    "clean_df = clean_df.join(F.broadcast(rate_ref_semi),\n",
    "                         clean_df.RatecodeID.cast(\"string\") == col(\"code_rate\"),\n",
    "                         \"left_semi\")\n",
    "\n",
    "# store_and_fwd_flag (allow NULLs: keep rows where flag matches OR flag is NULL)\n",
    "clean_df = clean_df.join(F.broadcast(store_ref_semi),\n",
    "                         (upper(clean_df.store_and_fwd_flag) == col(\"code_store\")) | clean_df.store_and_fwd_flag.isNull(),\n",
    "                         \"left_semi\")\n",
    "\n",
    "# payment_type\n",
    "clean_df = clean_df.join(F.broadcast(pay_ref_semi),\n",
    "                         clean_df.payment_type.cast(\"string\") == col(\"code_pay\"),\n",
    "                         \"left_semi\")\n",
    "\n",
    "print(\"Before refs filter:\", df.count(), \"→ After refs filter:\", clean_df.count())\n",
    "\n",
    "# 4) Simple sanity filters\n",
    "#    build duration, then keep realistic trips\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"trip_duration_min\",\n",
    "    F.round(\n",
    "        (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60.0,\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_df = (\n",
    "    clean_df\n",
    "    .filter(col(\"tpep_pickup_datetime\").isNotNull() & col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "    .filter(col(\"PULocationID\").isNotNull() & col(\"DOLocationID\").isNotNull())\n",
    "    .filter((col(\"trip_distance\") > 0) & (col(\"trip_distance\") < 100))          # (0, 100)\n",
    "    .filter((col(\"trip_duration_min\") >= 1) & (col(\"trip_duration_min\") <= 180)) # [1, 180] minutes\n",
    "    .filter((col(\"passenger_count\") >= 1) & (col(\"passenger_count\") <= 6))       # 1..6\n",
    "    .filter(col(\"fare_amount\")  >= 0)\n",
    "    .filter(col(\"tip_amount\")   >= 0)\n",
    "    .filter(col(\"total_amount\") > 0)\n",
    ")\n",
    "\n",
    "print(\"After sanity filters:\", clean_df.count())\n",
    "\n",
    "# =============================== #\n",
    "# -------- ADD THE LABELS ------- #\n",
    "# =============================== #\n",
    "\n",
    "# Build label slices (code + label) for LEFT joins\n",
    "pay_ref_lbl   = ref.filter(col(\"ref_type\")==\"payment_type\") \\\n",
    "                   .select(col(\"code\").alias(\"code_pay_lbl\"), col(\"label\").alias(\"payment_type_label\"))\n",
    "\n",
    "rate_ref_lbl  = ref.filter(col(\"ref_type\")==\"RatecodeID\") \\\n",
    "                   .select(col(\"code\").alias(\"code_rate_lbl\"), col(\"label\").alias(\"RatecodeID_label\"))\n",
    "\n",
    "vend_ref_lbl  = ref.filter(col(\"ref_type\")==\"VendorID\") \\\n",
    "                   .select(col(\"code\").alias(\"code_vendor_lbl\"), col(\"label\").alias(\"Vendor_label\"))\n",
    "\n",
    "store_ref_lbl = ref.filter(col(\"ref_type\")==\"store_and_fwd_flag\") \\\n",
    "                   .select(upper(col(\"code\")).alias(\"code_store_lbl\"), col(\"label\").alias(\"store_and_fwd_flag_label\"))\n",
    "\n",
    "# Join payment_type label\n",
    "if \"payment_type\" in clean_df.columns:\n",
    "    clean_df = (clean_df\n",
    "        .withColumn(\"payment_type_str\", col(\"payment_type\").cast(\"string\"))\n",
    "        .join(F.broadcast(pay_ref_lbl),\n",
    "              col(\"payment_type_str\") == col(\"code_pay_lbl\"),\n",
    "              \"left\")\n",
    "        .drop(\"code_pay_lbl\")\n",
    "    )\n",
    "\n",
    "# Join RatecodeID label\n",
    "if \"RatecodeID\" in clean_df.columns:\n",
    "    clean_df = (clean_df\n",
    "        .withColumn(\"RatecodeID_str\", col(\"RatecodeID\").cast(\"string\"))\n",
    "        .join(F.broadcast(rate_ref_lbl),\n",
    "              col(\"RatecodeID_str\") == col(\"code_rate_lbl\"),\n",
    "              \"left\")\n",
    "        .drop(\"code_rate_lbl\")\n",
    "    )\n",
    "\n",
    "# Join VendorID label\n",
    "if \"VendorID\" in clean_df.columns:\n",
    "    clean_df = (clean_df\n",
    "        .withColumn(\"VendorID_str\", col(\"VendorID\").cast(\"string\"))\n",
    "        .join(F.broadcast(vend_ref_lbl),\n",
    "              col(\"VendorID_str\") == col(\"code_vendor_lbl\"),\n",
    "              \"left\")\n",
    "        .drop(\"code_vendor_lbl\")\n",
    "    )\n",
    "\n",
    "# Join store_and_fwd_flag label\n",
    "if \"store_and_fwd_flag\" in clean_df.columns:\n",
    "    clean_df = (clean_df\n",
    "        .join(F.broadcast(store_ref_lbl),\n",
    "              upper(col(\"store_and_fwd_flag\")) == col(\"code_store_lbl\"),\n",
    "              \"left\")\n",
    "        .drop(\"code_store_lbl\")\n",
    "    )\n",
    "\n",
    "print(f\"After label joins: {clean_df.count()} rows\")\n",
    "\n",
    "# quick peek\n",
    "display(clean_df.select(\n",
    "    \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"trip_distance\",\"trip_duration_min\",\n",
    "    \"passenger_count\",\"fare_amount\",\"tip_amount\",\"total_amount\",\n",
    "    \"VendorID\",\"Vendor_label\",\n",
    "    \"RatecodeID\",\"RatecodeID_label\",\n",
    "    \"store_and_fwd_flag\",\"store_and_fwd_flag_label\",\n",
    "    \"payment_type\",\"payment_type_label\"\n",
    ").limit(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151fa732-b943-4c9e-bb34-3fd13d982e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc8f380-9c60-42d5-9ea3-2a73069d8aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================== #\n",
    "# -- Top 10 pickup zones per month (2024–2025) -- #\n",
    "# =============================================== #\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 0) add year/month if missing (based on pickup time)\n",
    "clean_df = (clean_df\n",
    "                .withColumn(\"year\",  F.year(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"month\", F.month(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# 1) aggregate trips by (year, month, PULocationID)\n",
    "by_month_zone = (\n",
    "    clean_df\n",
    "    .groupBy(\"year\", \"month\", \"PULocationID\")\n",
    "    .agg(F.count(\"*\").alias(\"nb_trips\"))\n",
    ")\n",
    "\n",
    "# 2) join zone lookup to get names/boroughs\n",
    "zones = (spark.table(\"hive_metastore.default.taxi_zone_lookup\")\n",
    "         .select(F.col(\"LocationID\").cast(\"int\").alias(\"LocationID\"),\n",
    "                 F.col(\"Borough\").alias(\"PU_Borough\"),\n",
    "                 F.col(\"Zone\").alias(\"PU_Zone\")))\n",
    "\n",
    "by_month_zone_named = (\n",
    "    by_month_zone\n",
    "    .join(zones, by_month_zone.PULocationID == zones.LocationID, \"left\")\n",
    "    .drop(\"LocationID\")\n",
    ")\n",
    "\n",
    "# 3) rank within each (year, month) and keep top 10\n",
    "w = Window.partitionBy(\"year\", \"month\").orderBy(F.col(\"nb_trips\").desc())\n",
    "\n",
    "top10_monthly = (\n",
    "    by_month_zone_named\n",
    "    .withColumn(\"rank\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    "    .orderBy(\"year\", \"month\", F.col(\"nb_trips\").desc())\n",
    ")\n",
    "\n",
    "display(top10_monthly.select(\"year\",\"month\",\"PULocationID\",\"PU_Borough\",\"PU_Zone\",\"nb_trips\",\"rank\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a580990-3635-4cf7-b135-307d0d504f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================== #\n",
    "# -- Average trip duration (minutes) per month -- #\n",
    "# =============================================== #\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "travel_duration_by_month = (\n",
    "    clean_df\n",
    "    .groupBy(F.year(\"tpep_pickup_datetime\").alias(\"year\"),\n",
    "             F.month(\"tpep_pickup_datetime\").alias(\"month\"))\n",
    "    .agg(F.round(F.avg(\"trip_duration_min\"), 2).alias(\"avg_trip_duration_min\"))\n",
    "    .orderBy(\"year\", \"month\")\n",
    ")\n",
    "\n",
    "display(travel_duration_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df3c8d1-bfc8-4ead-9cf9-45dc5250630d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================= #\n",
    "# --- Average trip distance by payment type --- #\n",
    "# ============================================= #\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) compute average distance per payment_type\n",
    "travel_distance_by_payment = (\n",
    "    clean_df\n",
    "    .groupBy(\"payment_type\")\n",
    "    .agg(F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_trip_distance\"))\n",
    "    .orderBy(F.col(\"avg_trip_distance\").desc())\n",
    ")\n",
    "\n",
    "# 2) get payment labels from ref table\n",
    "ref_df = (\n",
    "    spark.table(\"hive_metastore.default.taxi_universal_ref\")\n",
    "    .filter(F.col(\"ref_type\") == \"payment_type\")\n",
    "    .select(F.col(\"code\").alias(\"payment_type_code\"), F.col(\"label\").alias(\"payment_type_label\"))\n",
    ")\n",
    "\n",
    "# 3) join labels\n",
    "travel_distance_by_payment_labels = (\n",
    "    travel_distance_by_payment\n",
    "    .withColumn(\"payment_type_str\", F.col(\"payment_type\").cast(\"string\"))\n",
    "    .join(ref_df, F.col(\"payment_type_str\") == F.col(\"payment_type_code\"), \"left\")\n",
    "    .drop(\"payment_type_code\")\n",
    ")\n",
    "\n",
    "# 4) result\n",
    "display(travel_distance_by_payment_labels.select(\n",
    "    \"payment_type\", \"payment_type_label\", \"avg_trip_distance\"\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc406b27-04ff-4bdc-8bc4-675be8d3e725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================== #\n",
    "# --- Average fare amount by passenger count --- #\n",
    "# ============================================== #\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "average_fare_by_passenger_count = (\n",
    "    clean_df\n",
    "    .groupBy(\"passenger_count\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"nb_trips\"),\n",
    "        F.round(F.sum(\"fare_amount\"), 2).alias(\"total_fare_amount\"),\n",
    "        F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare_amount\"),\n",
    "        # total number of passengers\n",
    "        F.sum(\"passenger_count\").alias(\"total_passenger_count\"),\n",
    "        # average fare per passenger\n",
    "        F.round(F.sum(\"fare_amount\") / F.sum(\"passenger_count\"), 2).alias(\"avg_fare_per_passenger\")\n",
    "    )\n",
    "    .orderBy(\"passenger_count\")\n",
    ")\n",
    "\n",
    "display(average_fare_by_passenger_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac82080-87c3-45be-92a6-d380257e0ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================== #\n",
    "# ---------- Total tip amount per month -------- #\n",
    "# ============================================== #\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_tips_by_month = (\n",
    "    clean_df\n",
    "    .groupBy(\n",
    "        F.year(\"tpep_pickup_datetime\").alias(\"year\"),\n",
    "        F.month(\"tpep_pickup_datetime\").alias(\"month\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"nb_trips\"),\n",
    "        F.round(F.sum(\"tip_amount\"), 2).alias(\"total_tips\"),\n",
    "        F.round(F.avg(\"tip_amount\"), 2).alias(\"avg_tip_amount\")\n",
    "    )\n",
    "    .orderBy(\"year\", \"month\")\n",
    ")\n",
    "\n",
    "display(total_tips_by_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b2b7ea-c0d2-461b-b90f-0c213c269580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dbutils.secrets.listScopes())\n",
    "print(dbutils.secrets.list(\"local-scope\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9858c5d-d1de-4452-a1e9-adf2549f1f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================ #\n",
    "# ------------  Azure SQL Connection ------------- #\n",
    "# ================================================ #\n",
    "\n",
    "server   = \"cacfsql.database.windows.net\"\n",
    "database = \"brief\"\n",
    "user     = dbutils.secrets.get(\"local-scope\", \"jdbc_username\")\n",
    "password = dbutils.secrets.get(\"local-scope\", \"jdbc_password\")\n",
    "\n",
    "jdbc_url = (\n",
    "    f\"jdbc:sqlserver://{server}:1433;\"\n",
    "    f\"database={database};\"\n",
    "    \"encrypt=true;trustServerCertificate=false;\"\n",
    ")\n",
    "\n",
    "connection_props = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "\n",
    "def write_to_sql(df, table_name, mode=\"overwrite\", batchsize=10000):\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"jdbc\")\n",
    "          .option(\"url\", jdbc_url)\n",
    "          .option(\"dbtable\", table_name)\n",
    "          .option(\"batchsize\", batchsize)\n",
    "          .option(\"truncate\", \"true\" if mode==\"overwrite\" else \"false\")\n",
    "          .options(**connection_props)\n",
    "          .mode(mode)\n",
    "          .save()\n",
    "    )\n",
    "    print(f\"Table {table_name} written to Azure SQL ({mode})\")\n",
    "\n",
    "\n",
    "write_to_sql(top10_monthly,                    \"dbo.dcatry_01_top10_pickup_zones_monthly\")\n",
    "write_to_sql(travel_duration_by_month,         \"dbo.dcatry_02_avg_duration_by_month\")\n",
    "write_to_sql(travel_distance_by_payment_labels,\"dbo.dcatry_03_avg_distance_by_payment\")\n",
    "write_to_sql(average_fare_by_passenger_count,  \"dbo.dcatry_04_avg_fare_by_passenger_count\")\n",
    "write_to_sql(total_tips_by_month,              \"dbo.dcatry_05_total_tips_by_month\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb418fe-0754-4e94-95fa-2846662c12ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "# ---------------- Azure SQL Tables ---------------- #\n",
    "# ================================================== #\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# On réutilise ton url JDBC et les infos de connexion\n",
    "tables_df = (\n",
    "    spark.read\n",
    "         .format(\"jdbc\")\n",
    "         .option(\"url\", jdbc_url)\n",
    "         .option(\"query\", \"SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES\")\n",
    "         .options(**connection_props)\n",
    "         .load()\n",
    ")\n",
    "\n",
    "display(tables_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_TLC_TripRecordData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
